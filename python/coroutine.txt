本文是./Coroutine.pdf 的翻译&学习笔记。 

part 1 ： 介绍生成器和协程
	生成器generator：
		生成器是一个特殊的函数。普通函数返回一个值，而生成器函数则返回一系列值。
		调用生成器函数仅仅会创建并返回一个generator，不会运行该函数
		generator仅仅支持执行next()方法
			yield中止函数，并返回值
			当运行下一个next()后函数恢复运行，也就是从上一个yield中止的地方开始运行
		
	协程coroutine
		由来：
			在Python2.5开始，yield可以作为一个表达式expression
				expression:在编程中，表达式是表示值的符号的任何合法组合。 每个编程语言和应用程序都有自己的规则，什么是合法和非法的。 
					例如，在C语言中，x + 5是一个表达式，字符串“MONKEYS”也是这样。
			我们可以使用send()方法向生成器传递值，该值会被yield表达式捕获   
		协程类似于生成器，但他应用了yield可以接收传值的能力
			每个协程首先需要运行next()或者send(None)方法启动运行
			然后函数运行至yield处，准备接收值
		使用装饰器：
			这个装饰器主要是为了防止我们忘记运行next()
			def coroutine(func):
				def inner(*args,**kwargs):
					r = func(*args,**kwargs)
					next(r)
					return r
				return inner

			@coroutine
			def grep(pattern):
				print('pattern is %s'%pattern)
				input_str = 'sdkfkadfkj'
				while 1:
					input_str = yield input_str

			r = grep('python')
		关闭一个协程
			一个协程可能会无限期的运行，我们可以使用coroutine_obj.close()关闭它。注意：Python中的垃圾回收机制也会调用close()
			close()可以被捕捉，GeneratorExit
				@coroutine
				def grep(pattern):
					print('pattern is %s'%pattern)
					input_str = 'sdkfkadfkj'
					try:
						while 1:
							input_str = yield input_str
					except GeneratorExit:
						print('-------------end-------------')

				r = grep('python')
				r.close()
			
		抛出一个异常
			我们可以向coroutine中主动传递一个异常
			异常源于yield表达式
			可以用正常的方式捕获该异常
			
	协程与generator的区别：
		尽管有很多相似之处，但他俩是两个不同的概念
		generator主要目的是返回值
		而协程倾向于接收值
		协程和迭代器并无关联
		协程中可以使用yield产生一个值，但他不能与迭代绑定在一起
		
part 2 协程 管道 数据流

	处理管道
		协程可以被用来处理管道
			--send()-->| coroutine |--send()-> | coroutine |--send()-->| coroutine |-->
		只需要将协程串起来，并使用send()方法将数据传送进去
	管道源头：
		管道需要一个初始源(产生器)
			| source |--send()-->| coroutine |--send()-->
		这个产生器驱动整个管道
	管道槽 pipeline sinks
		管道需要一个终点
			--send()-->| coroutine |--send()-->| sink |
		收集所有数据send()至sink中，然后由sink来处理
	一个例子：
		import time
		def coroutine(func):

			def inner(*args,**kwargs):
				r = func(*args,**kwargs)
				next(r)
				return r
			return inner

		# A source 
		def follow(filename,target):
			with open(filename) as f:
				f.seek(0,0)
				while True:
					line = f.readline()
					if not line:
						time.sleep(1)
						continue
					target.send(line)

		# A sink that just prints the lines
		@coroutine
		def printer():
			while True:
				line = (yield)
				print(line)

		c = printer()
		follow('tail',c)
	解析该例子：
		| follow() | --send()--> | printer() |
		critical point： follow is driving entire computation by reading lines and pushing them into the printer() coroutine 
	管道过滤器 pipeline filters
		中间阶段既收又发
			--send()-->| coroutine |--send()-->
		通常执行某些数据转换，过滤，路由等
	过滤器例子
		# pipeline filters
		@coroutine
		def grep(pattern, target):
			while True:
				line = (yield).strip()
				if pattern in line:
					target.send(line)

		follow('tail',grep('hello',printer()))
		# 本例中  | follow() |--send()-->| grep() |--send()-->| printer() |
	插曲
		生成器使用迭代的方式从管道中拉取数据，而协程则使用send()的方式向管道中传递数据
		
	Being Branchy 分支
		with coroutine, you can send data to multiple destinations
			| source |--send()-->| coroutine |--send()-->| coroutine |--send()-->| coroutine |
											 |--send()-->| coroutine |--send()-->| coroutine |
											 |--send()-->| coroutine |--send()-->| coroutine |
		the source simply send data，further routing of that data could be arbitrarily complex
	broadcasting 广播
		向多个对象发送数据
			@coroutine
			def broadcast(targets):
				while True:
					line = (yield )
					for i in targets:
						i.send(line)
		this takes a sequence of coroutines and sends recived items to all of them 
		
	插曲
		coroutines privide more pwerful data routing possiblities than simple iterators
		如果您构建了一个简单的数据处理组件集合，您可以将它们粘合在一起成为管道，分支，合并等复杂的排列。
		although there are some limitation
	coroutines vs objects
		coroutines are somewhat similar to OO design patterns involving simple handler objects 协程有些相似涉及简单的处理对象面向对象的设计模式
			@coroutine
			def grep(pattern, target):
				while True:
					line = (yield).strip()
					if pattern in line:
						target.send(line)

			class grepHandler(object):
				def __init__(self, pattern, target):
					self.pattern = pattern
					self.target = target
				def send(self,line):
					if self.pattern in line:
						self.target.send(line)
		一个毫无疑问的简单概念
			协程是一种函数定义方法
		if you define a handler class
			你需要定义一个类
			定义两个方法
			可能还需要定义一个基类以及导入一个库
		coroutine更快
		coroutine和class都是创建对象，但是send时class需要在self中查找pattern和target。

part 3  coroutines and event dispatching协程和事件分派

	event handling 事件处理
		coroutines can be used to write various components that process event stream协程可以用来编写处理事件流的各个组件
		来一个例子：
	要解决的问题
		芝加哥交通管理局（CTA）为其大部分公共汽车配备实时GPS跟踪
		您可以在大街上的每一辆公共汽车上获取当前数据，作为一个大型XML文档
	XML解析
		有许多的方式来解析XML
		比较经典(老)的途径：SAX
		SAX是一个事件驱动的接口
	一些问题
		SAX可以使用较小的内存处理巨量的xml文件
		但是SAX的事件驱动的特性使他较为低级和尴尬
	xml sucks, dict rock!
		import xml.sax
		def coroutine(func):
			def inner(*args,**kwargs):
				r = func(*args,**kwargs)
				next(r)
				return r
			return inner

		class EventHandler(xml.sax.ContentHandler):
			def __init__(self,target):
				self.target = target
			def startElement(self, name, attrs):
				self.target.send(('start',(name,attrs._attrs)))
			def characters(self, content):
				self.target.send(('text',content))
			def endElement(self, name):
				self.target.send(('end',name))

		@coroutine
		def buses_to_dicts(target):
			while True:
				event, value = (yield)
				if event == 'start' and value[0] == 'bus':
					busdict = {}
					fragment = []
					while True:
						event, value = (yield)
						if event == 'start': fragment = []
						elif event == 'text': fragment.append(value)
						elif event == 'end':
							if value != 'bus':
								busdict[value] = ''.join(fragment)
							else:
								target.send(busdict)
								break

		@coroutine
		def filter_on_field(field,value,target):
			while True:
				d = (yield)
				if d.get(field) == value:
					target.send(d)

		@coroutine
		def bus_locations():
			while True:
				bus = (yield)
				print("%(route)s,%(id)s,\"%(direction)s\","\
				"%(latitude)s,%(longitude)s" % bus)

		xml.sax.parse("bus01.xml",
			EventHandler(
				buses_to_dicts(
						filter_on_field("route","22",
								filter_on_field("direction","North Bound",bus_locations())
						)
				)
		))

part 4  从数据处理到并发编程

	故事目前为止
		coroutines are similar to generator 
		您可以通过设置管道，数据流图表等来处理数据
		您可以创建小型处理组件的集合，并将它们连接在一起
		您可以使用具有棘手执行代码的协程（例如，事件驱动系统）
	一个通用的主题
		我们send数据到协程
		我们使用队列发送数据到线程
		我们使用messagees发送数据到进程
		协程自然的卷入到了线程和分布式系统
	基本并发
		你可以通过添加额外的层将协程包装进线程或者子进程
	协程与线程结合实例
		from threading import Thread
		import queue
		@coroutine
		def threaded(target):
			message = queue.Queue()
			def run_target():
				while True:
					item = message.get()		# 从队列中获取
					if item is GeneratorExit:	# 如果协程close()，则关闭操作的协程对象
						target.close()
						return
					else:
						target.send(item)
			Thread(target=run_target).start()	# 启动一个新的线程并运行
			try:
				while True:
					item = (yield )
					message.put(item)
			except GeneratorExit:
				message.put(GeneratorExit)
		
		xml.sax.parse("allroutes.xml", EventHandler(
			buses_to_dicts(
				threaded(
					filter_on_field("route","22", filter_on_field("direction","North Bound",bus_locations()))
				)
			)
		))
	执行vs执行环境
		使用协程，将任务的执行流程从执行环境中分离
		协程就是执行流程
		执行环境就是你所选择的线程、子进程……
	警告：
		大量的协程与线程或者进程的集合可能是一个比较好的编写非维护性应用的方式
		但这会导致你的应用运行起来很慢
		如果想要清楚的知道什么时候该这样用，你需要细心的学习该问题
	一些隐藏的危险：
		协程的send()方法必须正确的同步
		当一个协程正在执行时，你调用其send()方法，你的程序可能会崩溃
		比如：多线程发送数据到同一个目标协程
	局限性：
		不能将协程作为一个回路
			source-->coroutine-->coroutine
						|<-----------|
		堆栈的发送建立了一种调用栈（send()不返回，直到目标yield）
		如果你调用一个正在执行send()的协程，就会返回一个错误
		send()并不会暂停程序的执行

part 5 将协程作为任务

	任务的概念 task
		在并发编程中，通常将问题细分为“任务”
		任务几个基本的特性
			独立的控制流
			内部状态
			可以被调度(暂停/恢复)
			可以与其他任务通信
		声明：协程就是任务
	协程是否是任务
		首先 协程拥有自己的控制流
		其次 协程和其他Python函数一样，由一系列语句构成
		协程有自己的内部状态，比如说：局部变量，协程的局部变量的生命周期与协程的活动时间相同，他们共同建立一个执行环境
		协程可以进行通信，send()方法可以向协程中发送数据，yield表达式可以接收
		协程可以暂停和恢复，yield暂停执行，send()恢复执行，close()终止执行
	我们可以确信：
		很明显，协程看起来就是task
		但协程不是线程或者子进程
		一个问题：你可以在不使用这些概念的情况下执行多任务么？
		是否可以只使用协程实现多任务

part 6  操作系统的崩溃课程
	
	程序执行：
		在cpu上，程序是由一系列指令组成的
		当运行时，没有同时执行多任务的概念
	多任务：
		cpu对如何执行多任务一脸懵逼，应用程序也是一脸懵逼，那么，肯定有个东西知道多任务如何完成，这就是操作系统
	操作系统：
		操作系统（例如，Linux，Windows）负责在您的计算机上运行程序
		操作系统允许多个进程同时执行，它实现多进程同时执行的方式是快速切换
		那么：它是如何实现的？
	要点：
		当cpu运行你的程序时，操作系统并没有运行，那么问题来了：既然操作系统没有被运行，那么它是如何实现切换的呢？
	中断和陷阱
		操作系统通常有两种方式获得控制权：
			interrupt中断：某些硬件相关的信号(数据接收，时钟，按键)
			traps： 软件产生的信号
		当这两种情况的一种发生后，cpu会暂停当前进程，并运行内核进程，这时，内核可能会进行进程切换(俗称上下文切换)
	Traps and System Calls
		低级系统调用实际上是traps,这是一个特殊的cpu指令。当这个trap指令执行时，进程会暂停运行，然后内核会来接管cpu控制权
	任务调度：
		为了运行多个任务，内核维护了一个任务调度队列
	洞察
		yield 也是一种切换，当然并不是真正意义上的切换。
		当生成器函数命中“yield”语句时，它立即暂停执行
		控制权被传递回产生生成器函数的代码处
		如果将yield作为trap，我们可以构建一个多任务的“操作系统” - 当然必须在Python中
		
part 7  让我们构建一个os  ps：系上你的安全带

	我们的挑战目标：
		os必需是多任务处理
		使用纯Python开发
		不用线程或者子进程，使用协程
	动机：
		最近对线程的替代品很感兴趣
		比如说：非阻塞和异步I/O，服务器能够支持同一时刻数千个连接
		大量的工作集中在事件驱动或者 反应器模式(比如twisted)
		协程则与twisted完全不同
	步骤一： 定义任务
		定义一个task类，这个类用来封装coroutine
			# The tasks is the wrapper of the coroutines 
			class Task(object):
				taskid = 0
				def __init__(self, target):
					Task.taskid += 1		# 为每个task定义一个tid
					self.tid = Task.taskid
					self.target = target	# target指向coroutine对象
					self.sendval = None		
				def run(self):
					# 向coroutine中send()数据，并返回结果
					return self.target.send(self.sendval)
	步骤二：定义一个调度器
		class Scheduler(object):
			def __init__(self):
				self.ready = Queue()       # 一个队列，用来按照顺序存储task
				self.taskmap = {}
			
			def new(self,target):
				# 将coroutine进行封装，并加入调度列中
				newtask = Task(target)
				self.taskmap[newtask.tid] = newtask
				self.schedule(newtask)
				return newtask.tid
			# 将task放入队列中
			def schedule(self,task):
				self.ready.put(task)
			# 这是一个loop，不停的从队列中获取task并运行，然后将运行后的task放入队列
			def mainloop(self):
				while self.taskmap:
					task = self.ready.get()
					result = task.run()
					self.schedule(task)
		def foo():
			while True:
				print("I'm foo")
				yield
		def bar():
			while True:
				print("I'm bar")
				yield
				
		s = Scheduler()
		s.new(foo())
		s.new(bar())
		s.mainloop()
		yield的作用类似于一个trap，当某个task运行到了yield，调度器就会获得控制权，然后中断该task的运行，并调度其他task运行
		这样定义一个调度器会导致一个问题：当一个task运行结束，那么就会导致调度器崩溃
			def foo():
				for i in range(10):
					print("I'm foo")
					yield
	步骤三： 改进调度器
		class Scheduler(object):
			...
			# 将一个task从映射图中删除
			def exit(self,task):
				del self.taskmap[task.tid]
			# 这是一个loop，不停的从队列中获取task并运行，然后将运行后的task放入队列
			def mainloop(self):
				while self.taskmap:
					task = self.ready.get()
					try:
						result = task.run()
					except StopIteration:  # 当检测到某个task运行结束，则执行exit(task)方法，并重新从队列中获取值
						self.exit(task)
						continue
					self.schedule(task)
		在真正的操作系统中，traps是进程请求系统调用时发出的
		而在我们的os中，调度器作为os，yield作为trap,yield可以向调度器传值
		为了请求调度器的服务，任务将使用带有值的yield语句
	步骤四： 系统调用
		系统调用主要完成两个操作：
			1 执行相关操作
			2 调度该任务
		添加系统调用功能：

			class SystemCall(object):
				def handler(self):
					pass
			# 系统调用： 获取task的tid
			class GetTid(SystemCall):
				def handler(self):
					self.task.sendval = self.task.tid   # 第一步： 获取当前task的tid
					self.sched.schedule(self.task)      # 第二步： 调度该task

			class Scheduler(object):
				......
				# 这是一个loop，不停的从队列中获取task并运行，然后将运行后的task放入队列
				def mainloop(self):
					while self.taskmap:
						task = self.ready.get()
						try:
							result = task.run()
						except StopIteration:
							self.exit(task)
							continue
						# 如果yield返回的是一个SystemCall对象
						if isinstance(result,SystemCall):
							result.task = task      # 保存当前环境： 当前task
							result.sched = self     # 保存当前环境： 当前调度器
							result.handler()        # 调度器代表task运行系统调用
							continue
						self.schedule(task)
			def foo():
				mytid = yield GetTid()
				for i in range(5):
					print("I'm foo", mytid)
					yield
			def bar():
				mytid = yield GetTid()
				for i in range(10):
					print("I'm bar", mytid)
					yield
			s = Scheduler()
			s.new(foo())
			s.new(bar())
			s.mainloop()
		讨论：
			真正的操作系统具有强的“保护”概念（例如，存储器保护）
			应用程序和内核无强关联(traps仅仅是一个接口)
			我们要模仿这一点：
				tasks 看不到 调度器
				task 看不到其他 task 
				yield 是唯一的外部接口
	步骤五： task管理
		我们需要创建更多的系统调用
		创建一些task管理函数
			创建task
			杀死一个task
			等待task 退出
		创建task
			定义一个系统调用：
				class NewTask(SystemCall):
					def __init__(self,target):
						super().__init__()
						self.target = target

					def handler(self):
						# 第一步： 创建一个新的task
						tid = self.sched.new(self.target)
						self.task.sendval = tid
						# 第二步： 调度该task
						self.sched.schedule(self.task)
		kill task
			定义一个系统调用：
				class KillTask(SystemCall):

					def __init__(self,tid):
						'''
						:param tid:  要kill的task的tid
						'''
						super().__init__()
						self.tid = tid

					def handler(self):
						# 第一步： kill目标task
						task = self.sched.taskmap.get(self.tid,None)	
						if task:
							task.target.close()
							self.task.sendval = True
							self.sched.schdule(task)
							self.sched.exit(task)
						else:
							self.task.sendval = False
						# 第二步： 调度该task
						self.sched.schedule(self.task)
		Task waiting
			就是说，a task需要等待b task执行完成后才能继续运行
			先来讲一下大体思路：
				1 我们为调度程序设置一个字典，用来存储waiting task(相当于a task) 与 wait for task(相当于b task)的关系
				2 字典的键值对该如何设计呢？ 
					a方案： 假如是 a:[b1,b2...] 这样设计，那么a可以有很多的wait for task，但缺点就是当某个wait for task运行完后，
							我们需要遍历该字典，才能知道可以运行哪些task
					b方案： 假如是 b:[a1,a2...] 这样设计，那么当b完成后我们可以很快的知道哪些task可以运行了，缺点就是无法为a设置
							多个wait for task。(当然也可以设置多个，但很麻烦)
				3 我们偷个懒：每个task仅允许一个wait for task 。所以我们选择b方案。为调度器设立一个新的属性 self.exit_wating = {}
				4 当一个task运行完成后(也就是调度器执行了exit(task))，我们需要检查该task是否有关联的waiting task
					如果有的话： 将waiting task放入ready队列
					没有： do noting
				5 为调度器添加一个方法wait_for_exit：该方法接收两个参数：waiting_task wait_for_task_tid
					该方法会在调度器的exit_waiting中生成/修改对应的键值对
					返回是否成功
				5 创建一个系统调用 TaskWait
					该系统调用接受一个参数，接收wait for task的tid
					运行调度器的wait_for_exit方法
					若失败则调度该task
			设计讨论：
				task引用其他task的唯一方法是使用调度程序分配的整数task的tid
				这样进行封装很安全
				使得task相互分离
				他将所有的task管理存放在调度器中
		插曲：
			迄今为止，我们已经实现;
				多任务运行
				创建新task
				一些基本的task管理
			下一步：
				我们来实现一个web服务器，或者只是一个简单的echo服务器
		一个简单地echo服务器
			def handle_client(client, addr):
				print("Connection from", addr)
				while True:
					data = client.recv(65536)
					if not data:
						break
					client.send(data)
				client.close()
				print("Client closed")
				yield  # Make the function a generator/coroutine


			def server(port):
				print("Server starting")
				sock = socket.socket()
				sock.bind(("", port))
				sock.listen(5)
				while True:
					# 阻塞
					client, addr = sock.accept()
					yield NewTask(handle_client(client, addr))

			def alive():
				while True:
					print("I'm alive!")
					yield

			sched = Scheduler()
			sched.new(alive())
			sched.new(server(45000))
			sched.main_loop()
			'''
			输出：
				I'm alive!
				Server starting
				调度器锁定，停止运行其他task
			'''
			
			在真正的操作系统中，os会暂停整个Python解释器转而运行其他task。当I/O操作完成后才会调度该task
			而我们自己的操作系统显然很不尽如人意(任何阻塞操作都会冻结整个程序)
		非阻塞I/O 
			使用select模块，select可以监控多个文件描述符，我们可以获取那些满足某些条件(可读，可写……)的文件描述符。
			
	步骤6： I/O waiting
		1 我们决定使用select模块监控文件描述符
		2 定义I/O系统调用：
			class ReadWait(SystemCall):
				def __init__(self,f):
					self.f = f
				def handle(self):
					fd = self.f.fileno()
					self.sched.wait_for_read(self.task,fd)

			class WriteWait(SystemCall):
				def __init__(self,f):
					self.f = f
				def handle(self):
					fd = self.f.fileno()
					self.sched.wait_for_write(self.task,fd)
		3 为调度器添加两个新的属性,用来出处就绪的文件描述符
			    def __init__(self):
					...
					self.read_waiting = {}	# fd可读
					self.write_waiting = {}	# fd可写
			
		4 为调度器添加新的方法：
			class Scheduler(object):
				...
				def iopoll(self, timeout):
					'''
					检测并运行条件满足的文件描述符所对应的task
					:param timeout:  调用select时设定超时时间
					:return:  None
					'''
					if self.read_waiting or self.write_waiting:
						r, w, e = select.select(self.read_waiting,
												self.write_waiting, [], timeout)

						for fd in r: self.schedule(self.read_waiting.pop(fd))
						for fd in w: self.schedule(self.write_waiting.pop(fd))
						
				def wait_for_read(self, task, fd):
					self.read_waiting[fd] = task

				def wait_for_write(self, task, fd):
					self.write_waiting[fd] = task
		
		5  如何设置轮询iopoll？ 或者说iopoll方法该放在哪里？
			方式一：
				class Scheduler(object):
					...
					def mainloop(self):
						while self.taskmap:
							self.iopoll(0) # 放在这里的话，表示： 在每一次运行task之前都运行一次
							task = self.ready.get()
						try:
							result = task.run()
							...
				这样有些不太合理：
					运行self.iopoll()的频率是不是有些过于频繁了。如果队列中的task过多，就可能导致cpu资源浪费。
					每次运行select()，实际上会进行一次Python解释器与真正内核的上下文切换
			方式二：
				为调度器添加一个新的方法：
				    def iotask(self):
						while True:
							if self.ready.empty():
								self.iopoll(None)
							else:
								self.iopoll(0)
							yield
				修改mainloop()方法
					def mainloop(self):
						self.new(self.iotask()) # Launch I/O polls
						while self.taskmap:
							task = self.ready.get()
							...
				这样，就使得iopoll()和其他普通task一样，被调度运行
		6  修改echo服务器
			def handle_client(client,addr):
				print("Connection from", addr)
				while True:
					yield ReadWait(client)
					data = client.recv(65536)
					if not data:
						break
					yield WriteWait(client)
					client.send(data)
				client.close()
				print("Client closed")

			def server(port):
				print("Server starting")
				sock = socket.socket()
				sock.bind(("",port))
				sock.listen(5)
				while True:
					yield ReadWait(sock)
					client,addr = sock.accept()
					yield NewTask(handle_client(client,addr))
					
		7  运行一个实例：
			def alive():
				while True:
					print("I'm alive!")
					yield

			sched = Scheduler()
			sched.new(alive())
			sched.new(server(45000))
			sched.main_loop()
			
	wow！
		我们现在已经完成了一个多任务os
		他可以并发的执行任务
		task可以创建 销毁 wait for其他task
		task可以执行I/O操作 
		我们甚至可以可以根据这个os写一个并发服务器
		自舔一波吧，小伙子们！！
		
part 8  栈问题

	使用coroutine的限制：
		当使用协程时，我们无法写出yield（暂停）子例程的协程
		比如： 
			def Accept(sock):
				yield ReadWait(sock)   
				return sock.accept()	# yield与return无法共用！！！！
			def server(port):
				...
				while True:
					client,addr = Accept(sock)  # 这里错误
					yield NewTask(handle_client(client,addr))
	问题：
		yield语句只能用于在最高级别挂起的协程
		我们不能把yield传送到其他函数中
		就像这样：
			def bar():
				yield	# 此yield不会挂起调用自己(也就是bar())函数的“task”(即，它不挂起foo)
			def foo():
				bar()   
	
	解决方法：
		有一种方法来创建可挂起的子程序和函数，然而，它只能在任务调度器本身的帮助下完成，并且你必须严格遵守yield语句用法
		这个方法就是：trampolining 
		
	协程 Trampolining
		先来一个简单的例子：
			# A subroutine
			def add(x,y):
				yield x+y
			# A function that calls a subroutine
			def main():
				r = yield add(2,2)
				print(r)
				yield

			def run():
				m = main()
				# An example of a "trampoline"
				sub = m.send(None)  # sub 就是add(2,2)这个generator
				result = sub.send(None)    # result 就是(2+2)
				m.send(result)

			run()
			
	
			

			
		
				
				
		
			
		
		
		
	
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		

			
	